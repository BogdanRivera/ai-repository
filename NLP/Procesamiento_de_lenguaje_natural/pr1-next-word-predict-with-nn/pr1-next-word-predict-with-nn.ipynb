{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4560e096",
   "metadata": {},
   "source": [
    "# Predicción de palabras con redes neuronales \n",
    "\n",
    "Este proyecto se enfoca en la adquisición de un conjunton de datos para posteriormente introducir los embeddings en una red neuronal, permitiendo predecir la siguiente palabra basándose en dichas entradas. Para este caso se hace uso de un corpus de canciones (se pretende que sea únicamente en inglés) de un total de 79 generos musicales. Ese corpus contiene 379,893 letras de canciones de 4,239 artistas.\n",
    "\n",
    "\n",
    "Adquisición de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0194525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bugy1\\anaconda3\\envs\\pln\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\bugy1\\.cache\\kagglehub\\datasets\\neisse\\scrapped-lyrics-from-6-genres\\versions\\3\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"neisse/scrapped-lyrics-from-6-genres\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3762a01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Arerê</td>\n",
       "      <td>/ivete-sangalo/arere.html</td>\n",
       "      <td>Tudo o que eu quero nessa vida,\\nToda vida, é\\...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Se Eu Não Te Amasse Tanto Assim</td>\n",
       "      <td>/ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...</td>\n",
       "      <td>Meu coração\\nSem direção\\nVoando só por voar\\n...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Céu da Boca</td>\n",
       "      <td>/ivete-sangalo/chupa-toda.html</td>\n",
       "      <td>É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Quando A Chuva Passar</td>\n",
       "      <td>/ivete-sangalo/quando-a-chuva-passar.html</td>\n",
       "      <td>Quando a chuva passar\\n\\nPra quê falar\\nSe voc...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Sorte Grande</td>\n",
       "      <td>/ivete-sangalo/sorte-grande.html</td>\n",
       "      <td>A minha sorte grande foi você cair do céu\\nMin...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ALink                            SName  \\\n",
       "0  /ivete-sangalo/                            Arerê   \n",
       "1  /ivete-sangalo/  Se Eu Não Te Amasse Tanto Assim   \n",
       "2  /ivete-sangalo/                      Céu da Boca   \n",
       "3  /ivete-sangalo/            Quando A Chuva Passar   \n",
       "4  /ivete-sangalo/                     Sorte Grande   \n",
       "\n",
       "                                               SLink  \\\n",
       "0                          /ivete-sangalo/arere.html   \n",
       "1  /ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...   \n",
       "2                     /ivete-sangalo/chupa-toda.html   \n",
       "3          /ivete-sangalo/quando-a-chuva-passar.html   \n",
       "4                   /ivete-sangalo/sorte-grande.html   \n",
       "\n",
       "                                               Lyric language  \n",
       "0  Tudo o que eu quero nessa vida,\\nToda vida, é\\...       pt  \n",
       "1  Meu coração\\nSem direção\\nVoando só por voar\\n...       pt  \n",
       "2  É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...       pt  \n",
       "3  Quando a chuva passar\\n\\nPra quê falar\\nSe voc...       pt  \n",
       "4  A minha sorte grande foi você cair do céu\\nMin...       pt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "file_name = 'lyrics-data.csv'\n",
    "complete_route = path + '/' + file_name\n",
    "df = pd.read_csv(complete_route)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e3573",
   "metadata": {},
   "source": [
    "Verificar los lenguajes disponibles en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9919da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenguajes disponibles:  ['pt' 'es' 'en' nan 'it' 'gl' 'fr' 'de' 'tl' 'et' 'fi' 'pl' 'da' 'st' 'sv'\n",
      " 'ro' 'af' 'no' 'eu' 'rw' 'sw' 'ga' 'cy' 'ca' 'ny' 'ko' 'ar' 'gd' 'tr'\n",
      " 'id' 'su' 'lg' 'ru' 'nl' 'sq' 'is' 'cs' 'jw' 'lv' 'hu' 'ms' 'ku' 'zh'\n",
      " 'hr' 'ht' 'fa' 'mg' 'vi' 'ja' 'hmn' 'sr' 'iw' 'sl']\n"
     ]
    }
   ],
   "source": [
    "languages = df['language'].unique()\n",
    "print(\"Lenguajes disponibles: \",languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603f16e",
   "metadata": {},
   "source": [
    "En este proyecto únicamente se realizarán predicciones en inglés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abac3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Careless Whisper</td>\n",
       "      <td>/ivete-sangalo/careless-whisper.html</td>\n",
       "      <td>I feel so unsure\\nAs I take your hand and lead...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Could You Be Loved / Citação Musical do Rap: S...</td>\n",
       "      <td>/ivete-sangalo/could-you-be-loved-citacao-musi...</td>\n",
       "      <td>Don't let them fool, ya\\nOr even try to school...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Cruisin' (Part. Saulo)</td>\n",
       "      <td>/ivete-sangalo/cruisin-part-saulo.html</td>\n",
       "      <td>Baby, let's cruise, away from here\\nDon't be c...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Easy</td>\n",
       "      <td>/ivete-sangalo/easy.html</td>\n",
       "      <td>Know it sounds funny\\nBut, I just can't stand ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>For Your Babies (The Voice cover)</td>\n",
       "      <td>/ivete-sangalo/for-your-babies-the-voice-cover...</td>\n",
       "      <td>You've got that look again\\nThe one I hoped I ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ALink                                              SName  \\\n",
       "69   /ivete-sangalo/                                   Careless Whisper   \n",
       "86   /ivete-sangalo/  Could You Be Loved / Citação Musical do Rap: S...   \n",
       "88   /ivete-sangalo/                             Cruisin' (Part. Saulo)   \n",
       "111  /ivete-sangalo/                                               Easy   \n",
       "140  /ivete-sangalo/                  For Your Babies (The Voice cover)   \n",
       "\n",
       "                                                 SLink  \\\n",
       "69                /ivete-sangalo/careless-whisper.html   \n",
       "86   /ivete-sangalo/could-you-be-loved-citacao-musi...   \n",
       "88              /ivete-sangalo/cruisin-part-saulo.html   \n",
       "111                           /ivete-sangalo/easy.html   \n",
       "140  /ivete-sangalo/for-your-babies-the-voice-cover...   \n",
       "\n",
       "                                                 Lyric language  \n",
       "69   I feel so unsure\\nAs I take your hand and lead...       en  \n",
       "86   Don't let them fool, ya\\nOr even try to school...       en  \n",
       "88   Baby, let's cruise, away from here\\nDon't be c...       en  \n",
       "111  Know it sounds funny\\nBut, I just can't stand ...       en  \n",
       "140  You've got that look again\\nThe one I hoped I ...       en  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en = df[df['language'] == 'en']\n",
    "df_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c08e80",
   "metadata": {},
   "source": [
    "Se debe realizar una limpieza de los datos del apartado de Lyric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13b2b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bugy1\\AppData\\Local\\Temp\\ipykernel_5672\\3788646866.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_en['tokens'] = df_en['Lyric'].apply(preprocessing)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69     [i, feel, so, unsure, as, i, take, your, hand,...\n",
       "86     [do, n't, let, them, fool, ya, or, even, try, ...\n",
       "88     [baby, let, 's, cruise, away, from, here, do, ...\n",
       "111    [know, it, sounds, funny, but, i, just, ca, n'...\n",
       "140    [you, 've, got, that, look, again, the, one, i...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s\\']', ' ', text) \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df_en['tokens'] = df_en['Lyric'].apply(preprocessing)\n",
    "df_en['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922225a",
   "metadata": {},
   "source": [
    "Configuración de los tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "all_tokens = [token for sublist in df_en['tokens'].tolist() for token in sublist]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_en['tokens'].apply(' '.join))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) \n",
    "\n",
    "input_sequences = []\n",
    "for tokens in df_en['tokens']:\n",
    "    if len(tokens) > 1:\n",
    "        for i in range(1, len(tokens)):\n",
    "            n_gram_sequence = tokens[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = tokenizer.texts_to_sequences([' '.join(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "y_one_hot = np.zeros((len(y), total_words), dtype=np.int8)\n",
    "for i, word_idx in enumerate(y):\n",
    "    y_one_hot[i, word_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cd442",
   "metadata": {},
   "source": [
    "Realizar el modelo de red neuronal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06ca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "he\n",
      "hel\n",
      "hell\n",
      "hello\n",
      "world \n",
      "wo\n",
      "wor\n",
      "worl\n",
      "world\n",
      "world \n",
      "machine learning\n",
      "ma\n",
      "mac\n",
      "mach\n",
      "machi\n",
      "machin\n",
      "machine\n",
      "machine \n",
      "machine l\n",
      "machine le\n",
      "machine lea\n",
      "machine lear\n",
      "machine learn\n",
      "machine learni\n",
      "machine learnin\n",
      "machine learning\n",
      " learning python is great\n",
      " l\n",
      " le\n",
      " lea\n",
      " lear\n",
      " learn\n",
      " learni\n",
      " learnin\n",
      " learning\n",
      " learning \n",
      " learning p\n",
      " learning py\n",
      " learning pyt\n",
      " learning pyth\n",
      " learning pytho\n",
      " learning python\n",
      " learning python \n",
      " learning python i\n",
      " learning python is\n",
      " learning python is \n",
      " learning python is g\n",
      " learning python is gr\n",
      " learning python is gre\n",
      " learning python is grea\n",
      " learning python is great\n",
      "is\n",
      "is\n",
      "fun\n",
      "fu\n",
      "fun\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'hel',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'wo',\n",
       " 'wor',\n",
       " 'worl',\n",
       " 'world',\n",
       " 'world ',\n",
       " 'ma',\n",
       " 'mac',\n",
       " 'mach',\n",
       " 'machi',\n",
       " 'machin',\n",
       " 'machine',\n",
       " 'machine ',\n",
       " 'machine l',\n",
       " 'machine le',\n",
       " 'machine lea',\n",
       " 'machine lear',\n",
       " 'machine learn',\n",
       " 'machine learni',\n",
       " 'machine learnin',\n",
       " 'machine learning',\n",
       " ' l',\n",
       " ' le',\n",
       " ' lea',\n",
       " ' lear',\n",
       " ' learn',\n",
       " ' learni',\n",
       " ' learnin',\n",
       " ' learning',\n",
       " ' learning ',\n",
       " ' learning p',\n",
       " ' learning py',\n",
       " ' learning pyt',\n",
       " ' learning pyth',\n",
       " ' learning pytho',\n",
       " ' learning python',\n",
       " ' learning python ',\n",
       " ' learning python i',\n",
       " ' learning python is',\n",
       " ' learning python is ',\n",
       " ' learning python is g',\n",
       " ' learning python is gr',\n",
       " ' learning python is gre',\n",
       " ' learning python is grea',\n",
       " ' learning python is great',\n",
       " 'is',\n",
       " 'fu',\n",
       " 'fun']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Configuración\n",
    "embedding_dim = 100  # Tamaño del vector de cada palabra\n",
    "lstm_units = 150     # Neuronas en la capa LSTM\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=X.shape[1]))\n",
    "model.add(LSTM(lstm_units, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385b4ed",
   "metadata": {},
   "source": [
    "Entrenar modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y_one_hot, \n",
    "                    epochs=50, \n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 9: Función para predecir la siguiente palabra (SIMPLIFICADA)\n",
    "def predecir_siguiente_palabra(modelo, tokenizer, texto, num_predicciones=3):\n",
    "    \"\"\"\n",
    "    Predice la siguiente palabra dado un texto de entrada\n",
    "    \"\"\"\n",
    "    # 1. Limpiar y tokenizar el texto de entrada\n",
    "    tokens = texto.lower().split()  # Convertir a minúsculas y dividir en palabras\n",
    "    tokens = [palabra for palabra in tokens if palabra in tokenizer.word_index]\n",
    "    \n",
    "    if len(tokens) == 0:\n",
    "        return \"No hay palabras conocidas en el texto\"\n",
    "    \n",
    "    # 2. Convertir palabras a números (usando el tokenizer)\n",
    "    secuencia_numeros = tokenizer.texts_to_sequences([' '.join(tokens)])\n",
    "    \n",
    "    # 3. Rellenar la secuencia para que tenga el tamaño esperado por el modelo\n",
    "    secuencia_rellena = pad_sequences(secuencia_numerios, maxlen=X.shape[1], padding='pre')\n",
    "    \n",
    "    # 4. Hacer la predicción\n",
    "    prediccion = modelo.predict(secuencia_rellena, verbose=0)\n",
    "    \n",
    "    # 5. Obtener las palabras más probables\n",
    "    indices_top = np.argsort(prediccion[0])[-num_predicciones:][::-1]\n",
    "    \n",
    "    resultados = []\n",
    "    for indice in indices_top:\n",
    "        palabra = tokenizer.index_word.get(indice, '???')\n",
    "        probabilidad = prediccion[0][indice]\n",
    "        resultados.append((palabra, probabilidad))\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto_ejemplo = \"machine learning\"\n",
    "predicciones = predecir_siguiente_palabra(model, tokenizer, texto_ejemplo)\n",
    "\n",
    "print(f\"Para: '{texto_ejemplo}'\")\n",
    "print(\"Posibles siguientes palabras:\")\n",
    "for palabra, prob in predicciones:\n",
    "    print(f\"  {palabra} ({prob:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
